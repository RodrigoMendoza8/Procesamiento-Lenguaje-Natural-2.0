{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac688fb",
   "metadata": {},
   "source": [
    "# Rodrigo Mendoza Rodriguez SVM Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f6d8d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"author_profiling_pan.zip\", \"r\") as z:\n",
    "    z.extractall(\"carpeta_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "75d301ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n",
      "documents {}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('carpeta_docs/author_profiling_pan/es_test/ff011bde2e7212a3229d462b6809be9b.xml')\n",
    "root = tree.getroot()\n",
    "print(root.tag)\n",
    "for child in root:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b764265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    tokens = [tok for tok in tokens if tok not in stop_words]\n",
    "    return \" \".join(tokens)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "03efdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_texts_from_folder(path_folder):\n",
    "    tr_txt = []  # aquí van los documentos\n",
    "    tr_y = []    # aquí van las etiquetas\n",
    "    \n",
    "    for file in os.listdir(path_folder):\n",
    "        if file.endswith(\".xml\"):\n",
    "            tree = ET.parse(os.path.join(path_folder, file))\n",
    "            root = tree.getroot()\n",
    "            docs = []\n",
    "            for doc in root.iter(\"document\"): \n",
    "                texto_limpio = limpiar_texto(doc.text)   \n",
    "                if texto_limpio:  \n",
    "                    docs.append(texto_limpio)\n",
    "            tr_txt.append(\" \".join(docs))\n",
    "\n",
    "    truth_file = os.path.join(path_folder, \"truth.txt\")\n",
    "    if os.path.exists(truth_file):\n",
    "        with open(truth_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                tr_y.append(line.strip())\n",
    "    \n",
    "    return tr_txt, tr_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4fe124d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Personal Computer\\AppData\\Local\\Temp\\ipykernel_22652\\3924519435.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
      "C:\\Users\\Personal Computer\\AppData\\Local\\Temp\\ipykernel_22652\\3924519435.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  texto = BeautifulSoup(texto, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "path_test = 'carpeta_docs/author_profiling_pan/es_test/'\n",
    "path_train = 'carpeta_docs/author_profiling_pan/es_train/'\n",
    "tr_txt_test, tr_y_test = get_texts_from_folder(path_test)\n",
    "tr_txt_train, tr_y_train = get_texts_from_folder(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d6bb4cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a29115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Construir corpus de tokens\n",
    "corpus_de_palabras = []\n",
    "for doc in tr_txt_train:\n",
    "    corpus_de_palabras += doc.split()\n",
    "\n",
    "fdist = nltk.FreqDist(corpus_de_palabras)\n",
    "\n",
    "def sortFreqDist(freqDist):\n",
    "    aux = [(freqDist[key], key) for key in freqDist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux\n",
    "\n",
    "V = sortFreqDist(fdist)\n",
    "V = [word for word, _ in fdist.most_common(10000)]\n",
    "dict_indices = {word: i for i, word in enumerate(V)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "50270c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def built_bow_tr_binario(tr_txt, vocabulario, dict_indices):\n",
    "    # Objetivo: Construir la matriz de bow\n",
    "    bow = np.zeros((len(tr_txt), len(vocabulario)), dtype=np.int8)\n",
    "    cont_doc = 0\n",
    "    # Rellenar la matriz, cada renglon es un twitt y cada renglon una palabra\n",
    "    for tr in tr_txt:\n",
    "        if not tr:  # ignora vacíos\n",
    "            continue\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(str(tr).lower()))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                bow[cont_doc, dict_indices[word]] = 1\n",
    "        cont_doc += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d069ee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tr = built_bow_tr_binario(tr_txt_train, V, dict_indices)\n",
    "bow_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1860e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200, 10000)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8a07c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argentina': 0,\n",
       " 'chile': 1,\n",
       " 'colombia': 2,\n",
       " 'mexico': 3,\n",
       " 'peru': 4,\n",
       " 'spain': 5,\n",
       " 'venezuela': 6}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paises = []\n",
    "for index, renglon in enumerate(tr_y_test):\n",
    "    labels = tr_y_test[index].split(':::')\n",
    "    if labels[2] not in paises:\n",
    "        paises.append(labels[2])\n",
    "paises_numericas_test = {}\n",
    "for index, pais in enumerate(paises):\n",
    "    paises_numericas_test[pais] = index\n",
    "paises_numericas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0773c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un set con todos los países (train + test)\n",
    "paises = set()\n",
    "\n",
    "with open(path_train + \"/truth.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        labels = line.strip().split(\":::\")\n",
    "        paises.add(labels[2])\n",
    "\n",
    "with open(path_test + \"/truth.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        labels = line.strip().split(\":::\")\n",
    "        paises.add(labels[2])\n",
    "\n",
    "# Ahora asignamos un número fijo a cada país\n",
    "paises_numericas = {pais: idx for idx, pais in enumerate(sorted(paises))}\n",
    "\n",
    "# Usar SIEMPRE este mismo diccionario para codificar\n",
    "y_train = []\n",
    "with open(path_train + \"/truth.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        labels = line.strip().split(\":::\")\n",
    "        y_train.append(paises_numericas[labels[2]])\n",
    "\n",
    "y_test = []\n",
    "with open(path_test + \"/truth.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        labels = line.strip().split(\":::\")\n",
    "        y_test.append(paises_numericas[labels[2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236befc",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4ab53b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "50983460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17 14 18 22 16 19 14]\n",
      " [21 20 15 13 23 15 13]\n",
      " [20 24 17 12 20 10 17]\n",
      " [17 26 21 19 11 17  9]\n",
      " [16 11 21 20 17 15 20]\n",
      " [13 19 23  5 23 17 20]\n",
      " [18 15 19 22 17 13 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.14      0.14       120\n",
      "           1       0.16      0.17      0.16       120\n",
      "           2       0.13      0.14      0.13       120\n",
      "           3       0.17      0.16      0.16       120\n",
      "           4       0.13      0.14      0.14       120\n",
      "           5       0.16      0.14      0.15       120\n",
      "           6       0.15      0.13      0.14       120\n",
      "\n",
      "    accuracy                           0.15       840\n",
      "   macro avg       0.15      0.15      0.15       840\n",
      "weighted avg       0.15      0.15      0.15       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train80, X_val20, y_train80, y_val20 = train_test_split(\n",
    "    bow_tr, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=4, scoring='f1_macro', cv=5)\n",
    "grid.fit(X_train80, y_train80)\n",
    "y_pred = grid.predict(X_val20)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_val20, y_pred, average='macro')\n",
    "\n",
    "print(confusion_matrix(y_val20, y_pred))\n",
    "print(metrics.classification_report(y_val20, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f123c",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "95bb76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def built_bow_tr_frecuencia(tr_txt, vocabulario, dict_indices):\n",
    "    # Objetivo: Construir la matriz de bow\n",
    "    bow = np.zeros((len(tr_txt), len(vocabulario)), dtype=int)\n",
    "    cont_doc = 0\n",
    "    # Rellenar la matriz, cada renglon es un twitt y cada renglon una palabra\n",
    "    for tr in tr_txt:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr.lower()))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                bow[cont_doc, dict_indices[word]] = fdist_doc[word]\n",
    "        cont_doc +=1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "838393d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "corpus_de_palabras_f = []\n",
    "for doc in tr_txt_train:\n",
    "    if isinstance(doc, str):              \n",
    "        corpus_de_palabras_f += tokenizer.tokenize(doc)\n",
    "    \n",
    "fdist = nltk.FreqDist(corpus_de_palabras)\n",
    "def sortFreqDist(freqDist):\n",
    "    aux = [(freqDist[key], key) for key in freqDist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux\n",
    "V_f = sortFreqDist(fdist)\n",
    "V_f = V[:10000]\n",
    "dict_indices_f = dict()\n",
    "\n",
    "cont = 0\n",
    "for weight, word in V_f:\n",
    "    dict_indices_f[word] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cc1bf6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1 34 ...  0  0  0]\n",
      " [ 0  1 26 ...  0  0  0]\n",
      " [ 0  2 31 ...  0  0  0]\n",
      " ...\n",
      " [ 0  1 21 ...  0  0  0]\n",
      " [ 0  0 24 ...  0  0  0]\n",
      " [ 0  0 23 ...  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4200, 1805)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_frecuencia = built_bow_tr_frecuencia(tr_txt_train,V_f,dict_indices_f)\n",
    "print(bow_train_frecuencia)\n",
    "bow_train_frecuencia.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3b2ed50f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[171], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m svr \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mLinearSVC(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39msvr, param_grid\u001b[38;5;241m=\u001b[39mparameters, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m grid\u001b[38;5;241m.\u001b[39mfit(X_train80, y_train80)\n\u001b[0;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mpredict(X_val20)\n\u001b[0;32m     11\u001b[0m p, r, f, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(y_val20, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    971\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    972\u001b[0m         clone(base_estimator),\n\u001b[0;32m    973\u001b[0m         X,\n\u001b[0;32m    974\u001b[0m         y,\n\u001b[0;32m    975\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    976\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    977\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    978\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    979\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    981\u001b[0m     )\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    983\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    985\u001b[0m     )\n\u001b[0;32m    986\u001b[0m )\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train80, X_val20, y_train80, y_val20 = train_test_split(\n",
    "    bow_train_frecuencia, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=4, scoring='f1_macro', cv=5)\n",
    "grid.fit(X_train80, y_train80)\n",
    "y_pred = grid.predict(X_val20)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_val20, y_pred, average='macro')\n",
    "\n",
    "print(confusion_matrix(y_val20, y_pred))\n",
    "print(metrics.classification_report(y_val20, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97937ecc",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46cdf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "19 fits failed out of a total of 35.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "19 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 305, in fit\n",
      "    X, y = validate_data(\n",
      "           ~~~~~~~~~~~~~^\n",
      "        self,\n",
      "        ^^^^^\n",
      "    ...<5 lines>...\n",
      "        accept_large_sparse=False,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "        X,\n",
      "    ...<12 lines>...\n",
      "        input_name=\"X\",\n",
      "    )\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "  File \"c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 839, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 1.00 GiB for an array with shape (2688, 50000) and data type float64\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Personal Computer\\instalar_anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 13 15 18 26 10 18]\n",
      " [21 14 17 18 20 15 15]\n",
      " [21 12 15 19 17 15 21]\n",
      " [24  6 27 13 14 16 20]\n",
      " [16 10 20 19 18 16 21]\n",
      " [21 13 20 13 23 16 14]\n",
      " [17 16 16 16 20 16 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.17      0.15       120\n",
      "           1       0.17      0.12      0.14       120\n",
      "           2       0.12      0.12      0.12       120\n",
      "           3       0.11      0.11      0.11       120\n",
      "           4       0.13      0.15      0.14       120\n",
      "           5       0.15      0.13      0.14       120\n",
      "           6       0.15      0.16      0.15       120\n",
      "\n",
      "    accuracy                           0.14       840\n",
      "   macro avg       0.14      0.14      0.14       840\n",
      "weighted avg       0.14      0.14      0.14       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "bow_tr = bow_tr.astype(np.float32)\n",
    "bow_train_L2 = normalize(bow_tr, norm='l2')\n",
    "X_train80, X_val20, y_train80, y_val20 = train_test_split(\n",
    "    bow_train_L2, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=4, scoring='f1_macro', cv=5)\n",
    "grid.fit(X_train80, y_train80)\n",
    "y_pred = grid.predict(X_val20)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_val20, y_pred, average='macro')\n",
    "\n",
    "print(confusion_matrix(y_val20, y_pred))\n",
    "print(metrics.classification_report(y_val20, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8877ef",
   "metadata": {},
   "source": [
    "# Ejercicio 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 12 17 24 29 10 15]\n",
      " [20  7 17 33 22 11 10]\n",
      " [11 12 23 25 15 11 23]\n",
      " [29  6 20 21 17 12 15]\n",
      " [22  7 20 23 22 16 10]\n",
      " [23  4 25 26 19 10 13]\n",
      " [23 12 17 21 18  9 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.11      0.10       120\n",
      "           1       0.12      0.06      0.08       120\n",
      "           2       0.17      0.19      0.18       120\n",
      "           3       0.12      0.17      0.14       120\n",
      "           4       0.15      0.18      0.17       120\n",
      "           5       0.13      0.08      0.10       120\n",
      "           6       0.19      0.17      0.18       120\n",
      "\n",
      "    accuracy                           0.14       840\n",
      "   macro avg       0.14      0.14      0.13       840\n",
      "weighted avg       0.14      0.14      0.13       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "bow_tr = bow_tr.astype(np.float32)\n",
    "bow_train_frecuencia_L2 = normalize(bow_train_frecuencia, norm='l2')\n",
    "X_train80, X_val20, y_train80, y_val20 = train_test_split(\n",
    "    bow_train_frecuencia_L2, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=4, scoring='f1_macro', cv=5)\n",
    "grid.fit(X_train80, y_train80)\n",
    "y_pred = grid.predict(X_val20)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_val20, y_pred, average='macro')\n",
    "\n",
    "print(confusion_matrix(y_val20, y_pred))\n",
    "print(metrics.classification_report(y_val20, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f678c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
